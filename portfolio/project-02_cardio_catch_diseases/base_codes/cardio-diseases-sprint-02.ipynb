{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aging-taste",
   "metadata": {},
   "source": [
    "### **D2APR: Aprendizado de M√°quina e Reconhecimento de Padr√µes** (IFSP, Campinas) <br/>\n",
    "**Prof**: Samuel Martins (Samuka) <br/>\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>. <br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-particle",
   "metadata": {},
   "source": [
    "#### Custom CSS style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-saudi",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".dashed-box {\n",
    "    border: 1px dashed black !important;\n",
    "#    font-size: var(--jp-content-font-size1) !important;\n",
    "}\n",
    "\n",
    ".dashed-box table {\n",
    "\n",
    "}\n",
    "\n",
    ".dashed-box tr {\n",
    "    background-color: white !important;\n",
    "}\n",
    "        \n",
    ".alt-tab {\n",
    "    background-color: black;\n",
    "    color: #ffc351;\n",
    "    padding: 4px;\n",
    "    font-size: 1em;\n",
    "    font-weight: bold;\n",
    "    font-family: monospace;\n",
    "}\n",
    "// add your CSS styling here\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-enterprise",
   "metadata": {},
   "source": [
    "<span style='font-size: 2.5em'><b>Cardiovascular Disease üíî</b></span><br/>\n",
    "<span style='font-size: 1.5em'>Predict cardiovascular diseases</span>\n",
    "\n",
    "<span style=\"background-color: #ffc351; padding: 4px; font-size: 1em;\"><b>Sprint #2</b></span>\n",
    "\n",
    "<img src=\"./imgs/cardio.png\" width=300/>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-carpet",
   "metadata": {},
   "source": [
    "## Before starting this notebook\n",
    "This jupyter notebook is designed for **experimental and teaching purposes**. <br/>\n",
    "Although it is (relatively) well organized, it aims at solving the _target problem_ by evaluating (and documenting) _different solutions_ for somes steps of the **machine learning pipeline** ‚Äî see the [***Machine Learning Project Checklist by xavecoding***](https://github.com/xavecoding/IFSP-CMP-D2APR-2021.2/blob/main/cheat-sheets/machine-learning-project-checklist_by_xavecoding.pdf). <br/>\n",
    "We tried to make this notebook as literally a _notebook_. Thus, it contains notes, drafts, comments, etc.<br/>\n",
    "\n",
    "For teaching purposes, some parts of the notebook may be _overcommented_. Moreover, to simulate a real development scenario, we will divide our solution and experiments into **\"sprints\"** in which each sprint has some goals (e.g., perform _feature selection_, train more ML models, ...). <br/>\n",
    "The **sprint goal** will be stated at the beginning of the notebook.\n",
    "\n",
    "A ***final notebook*** (or any other kind of presentation) that compiles and summarizes all sprints ‚Äî the target problem, solutions, and findings ‚Äî should be created later.\n",
    "\n",
    "#### Conventions\n",
    "\n",
    "<ul>\n",
    "    <li>üí° indicates a tip. </li>\n",
    "    <li> ‚ö†Ô∏è indicates a warning message. </li>\n",
    "    <li><span class='alt-tab'>alt tab</span> indicates and an extra content (<i>e.g.</i>, slides) to explain a given concept.</li>\n",
    "</ul>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-convert",
   "metadata": {},
   "source": [
    "## üéØ Sprint Goals\n",
    "- Preprocess the data\n",
    "- Evaluate on the training set: KNN, Logistic Regression, and Polynomial Logistic Regression\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-apollo",
   "metadata": {},
   "source": [
    "### 0. Imports and default settings for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-morrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "          'figure.figsize': (15, 5),\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-beatles",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è 5. Prepare the Data\n",
    "\n",
    "**Preprocessing tasks**\n",
    "- Fill in missing values (imputation)\n",
    "- Add new features\n",
    "- Feature Scaling\n",
    "- One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-fifteen",
   "metadata": {},
   "source": [
    "### 5.1. Load the cleaned training set\n",
    "Let's consider the training and testing sets already cleaned (Sprint #1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-values",
   "metadata": {},
   "outputs": [],
   "source": [
    "cardio_train = pd.read_csv('./datasets/cardio_clean_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-commissioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "cardio_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-extension",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to remember what categorical variables are like\n",
    "for cat_attribute in ['gender', 'cholesterol', 'gluc', 'smoke', 'alco', 'active']:\n",
    "    print(cardio_train[cat_attribute].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-atlas",
   "metadata": {},
   "source": [
    "### 5.2. Separate the features and the classes (target outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-bathroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "cardio_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-iraqi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the target outcome into a numpy array\n",
    "y_train = cardio_train['cardio'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-deviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-congress",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwrite the dataframe with only the features  \n",
    "cardio_train = cardio_train.drop(columns=['cardio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-gospel",
   "metadata": {},
   "outputs": [],
   "source": [
    "cardio_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-sigma",
   "metadata": {},
   "outputs": [],
   "source": [
    "cardio_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-differential",
   "metadata": {},
   "source": [
    "### 5.3. Separate the numerical and categorical features¬∂\n",
    "Since we perform different preprocessing tasks (transformations) to _numerical features_ and _categorical ones_, let's split them into two different dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-samuel",
   "metadata": {},
   "outputs": [],
   "source": [
    "cardio_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-transcription",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical variables\n",
    "num_vars = ['age', 'height', 'weight', 'ap_hi', 'ap_lo']\n",
    "\n",
    "# categorical binary variables\n",
    "bin_vars = ['gender', 'smoke', 'alco', 'active']\n",
    "\n",
    "# categorical variables\n",
    "cat_vars = ['cholesterol', 'gluc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-heather",
   "metadata": {},
   "outputs": [],
   "source": [
    "## separating the features into specific dataset according to their type\n",
    "cardio_train_num = cardio_train[num_vars]\n",
    "cardio_train_bin = cardio_train[bin_vars]\n",
    "cardio_train_cat = cardio_train[cat_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-rendering",
   "metadata": {},
   "source": [
    "### 5.4. Creating Preprocessing Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-microwave",
   "metadata": {},
   "source": [
    "#### **Standard Preprocessing Pipeline**\n",
    "Not suitable for polinomial-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-ethics",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('robust_scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "bin_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent'))  # as the categories are numbers, we can use the SimpleImputer\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # as the categories are numbers, we can use the SimpleImputer\n",
    "    ('one-hot-encoding', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# (name, transformer, columns)\n",
    "preprocessed_pipeline = ColumnTransformer([\n",
    "    ('numerical', num_pipeline, num_vars),\n",
    "    ('binary', bin_pipeline, bin_vars),\n",
    "    ('categorical', cat_pipeline, cat_vars)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-parameter",
   "metadata": {},
   "source": [
    "#### **Preprocessing Pipeline for Polynomial Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-swing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "pol_num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('poly_feat_transformer', PolynomialFeatures(include_bias=False)),  # default degree = 2\n",
    "    ('robust_scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "bin_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent'))  # as the categories are numbers, we can use the SimpleImputer\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # as the categories are numbers, we can use the SimpleImputer\n",
    "    ('one-hot-encoding', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "\n",
    "# (name, transformer, columns)\n",
    "polynomial_preprocessed_pipeline = ColumnTransformer([\n",
    "    ('numerical', pol_num_pipeline, num_vars),\n",
    "    ('binary', bin_pipeline, bin_vars),\n",
    "    ('categorical', cat_pipeline, cat_vars)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-atmosphere",
   "metadata": {},
   "source": [
    "### üèãÔ∏è‚Äç‚ôÄÔ∏è 6. Train ML Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-harris",
   "metadata": {},
   "source": [
    "#### 6.1. Getting the independent (features) and classes (outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-newport",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard pipeline (for KNN and Logistic Regression)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-collectible",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing pipeline for polynomial-based methods (Polynomial Logistic Regression)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-sarah",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we already have y_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-weekly",
   "metadata": {},
   "source": [
    "### 6.2. Training the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-transmission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing function\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"\\nMean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-capital",
   "metadata": {},
   "source": [
    "#### **KNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-commitment",
   "metadata": {},
   "source": [
    "##### **Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-huntington",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "suspended-second",
   "metadata": {},
   "source": [
    "#### **Logistic Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-miniature",
   "metadata": {},
   "source": [
    "##### **Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-limit",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "young-wisdom",
   "metadata": {},
   "source": [
    "#### **Polynomial Logistic Regression (degree=2)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-ownership",
   "metadata": {},
   "source": [
    "##### **Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-canvas",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
