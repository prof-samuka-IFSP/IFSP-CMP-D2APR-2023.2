{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ambient-blake",
   "metadata": {},
   "source": [
    "### **D2APR: Aprendizado de M√°quina e Reconhecimento de Padr√µes** (IFSP, Campinas) <br/>\n",
    "**Prof**: Samuel Martins (Samuka) <br/>\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>. <br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-joint",
   "metadata": {},
   "source": [
    "### Custom CSS style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-energy",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".dashed-box {\n",
    "    border: 1px dashed black !important;\n",
    "}\n",
    ".dashed-box tr {\n",
    "  background-color: white !important;  \n",
    "}\n",
    ".alt-tab {\n",
    "    background-color: black;\n",
    "    color: #ffc351;\n",
    "    padding: 4px;\n",
    "    font-size: 1em;\n",
    "    font-weight: bold;\n",
    "    font-family: monospace;\n",
    "}\n",
    "// add your CSS styling here\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-lexington",
   "metadata": {},
   "source": [
    "<span style='font-size: 2.5em'><b>California Housing üè°</b></span><br/>\n",
    "<span style='font-size: 1.5em'>Predict the median housing price in California districts</span>\n",
    "\n",
    "<span style=\"background-color: #ffc351; padding: 4px; font-size: 1em;\"><b>Sprint 4</b></span>\n",
    "\n",
    "<img src=\"./imgs/california-flag.png\" width=300/>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-fossil",
   "metadata": {},
   "source": [
    "## Before starting this notebook\n",
    "This jupyter notebook is designed for **experimental and teaching purposes**. <br/>\n",
    "Although it is (relatively) well organized, it aims at solving the _target problem_ by evaluating (and documenting) _different solutions_ for somes steps of the **machine learning pipeline** ‚Äî see the ***Machine Learning Project Checklist by xavecoding***. <br/>\n",
    "We tried to make this notebook as literally a _notebook_. Thus, it contains notes, drafts, comments, etc.<br/>\n",
    "\n",
    "For teaching purposes, some parts of the notebook may be _overcommented_. Moreover, to simulate a real development scenario, we will divide our solution and experiments into **\"sprints\"** in which each sprint has some goals (e.g., perform _feature selection_, train more ML models, ...). <br/>\n",
    "The **sprint goal** will be stated at the beginning of the notebook.\n",
    "\n",
    "A ***final notebook*** (or any other kind of presentation) that compiles and summarizes all sprints ‚Äî the target problem, solutions, and findings ‚Äî should be created later.\n",
    "\n",
    "#### Conventions\n",
    "\n",
    "<ul>\n",
    "    <li>üí° indicates a tip. </li>\n",
    "    <li> ‚ö†Ô∏è indicates a warning message. </li>\n",
    "    <li><span class='alt-tab'>alt tab</span> indicates and an extra content (<i>e.g.</i>, slides) to explain a given concept.</li>\n",
    "</ul>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-nelson",
   "metadata": {},
   "source": [
    "## üéØ Sprint Goals\n",
    "- Refactor our codes by using the sklearn Pipelines\n",
    "- Evaluate the models in the Test Set\n",
    "- Compare the models with the baseline\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-trader",
   "metadata": {},
   "source": [
    "### 0. Imports and default settings for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-governor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "          'figure.figsize': (15, 5),\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-universe",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è 5. Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-narrow",
   "metadata": {},
   "source": [
    "#### **Preprocessing tasks**\n",
    "- Fill in missing values (imputation)\n",
    "- Add new features\n",
    "- Feature Scaling\n",
    "- One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-gibraltar",
   "metadata": {},
   "source": [
    "<table align=\"left\" class=\"dashed-box\">\n",
    "<tr>\n",
    "    <td><span class='alt-tab'>alt tab</span></td>\n",
    "    <td><b>Slides:</b> Scikit-Learn Design Principles - Hyperparameters vs Parameters<br/>\n",
    "        <b>Slides:</b> Scikit-Learn Design Principles - Main APIs</td>\n",
    "</tr>\n",
    "</table><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-jordan",
   "metadata": {},
   "source": [
    "### 5.1. Load the cleaned training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-powder",
   "metadata": {},
   "source": [
    "Let's consider the training and testing sets already cleaned (sprint #2):\n",
    "- Drop duplicated instances (no found)\n",
    "- Drop instances with `housing_median_age` capped at 52\n",
    "- Drop instances with `median_house_value` capped at 500001.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-sheep",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the cleaned training set\n",
    "housing_train = pd.read_csv('./datasets/housing_train_sprint-2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-mustang",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-carbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-hardwood",
   "metadata": {},
   "source": [
    "### 5.2. Separate the _features_ and the _target outcome_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fourth-centre",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-marijuana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the target outcome into a numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-frederick",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-mailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwrite the dataframe with only the features  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-albuquerque",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-cruise",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-major",
   "metadata": {},
   "source": [
    "### 5.3. Separate the _numerical_ and _categorical_ features\n",
    "Since we perform different preprocessing tasks (transformations) to _numerical_ features and _categorical_ ones, let's split them into two different dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-intranet",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-hollow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical atributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-luxury",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-sweden",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating the features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-boost",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-bible",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-ceiling",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.4. Filling in missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-reference",
   "metadata": {},
   "source": [
    "`sklearn.impute.SimpleImputer` <br/>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-blank",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-director",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.statistics_  # computed medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-naples",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_num.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-portable",
   "metadata": {},
   "source": [
    "<table align=\"left\" class=\"dashed-box\">\n",
    "<tr>\n",
    "    <td>üí°</td>\n",
    "    <td>The <code>SimpleImputer</code> finds out the <i>statistic for imputation</i> <b>for ALL features</b>.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td>We can save this <i>transformer</i> on the disk for future transfomations.</td>\n",
    "</tr>\n",
    "</table><br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-disposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling in the missing values FOR ALL attributes\n",
    "# it generates a numpy array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-nutrition",
   "metadata": {},
   "source": [
    "### 5.5. Adding new features\n",
    "To _automate data preprocessing_ via sklearn, we will need _to create_ our **own transformer** to add the new features considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "# template to create an own estimation\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class NameOfYourTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return None  # return the transformed data instead of None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-muscle",
   "metadata": {},
   "source": [
    "Since our custom transformer can be executed before other transformation, we will consider that the input is a **numpy 2D array**, not a _dataframe_. <br/>\n",
    "\n",
    "This transformer will create 3 new features, based on the current ones:\n",
    "- `total_rooms`\n",
    "- `total_bedrooms`\n",
    "- `population`\n",
    "- `households`\n",
    "\n",
    "\n",
    "Thus, we need to find their column indices first because our input will be a **numpy 2D array**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-offset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the integer index of each attribute/column:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-birthday",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-heating",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_engineer = HousingFeatEngineering()\n",
    "\n",
    "housing_train_num_new_feats = feat_engineer.transform(housing_train_num.values)  # we need to convert it to numpy first\n",
    "housing_train_num_new_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-actress",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_num_new_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-simulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the new feats\n",
    "housing_train_num_new_feats[:, -3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-combine",
   "metadata": {},
   "source": [
    "### 5.6. Feature Scaling\n",
    "Exactly as performed in the previous sprint: **RobustScaler**. <br/>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-metropolitan",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(housing_train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-viewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_num_scaled = scaler.transform(housing_train_num)\n",
    "housing_train_num_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removable-picture",
   "metadata": {},
   "source": [
    "### 5.7. Categorical Variable Encoding\n",
    "Instead of using the method `.get_dummies()` from _pandas_, let's use a method from _sklearn_.\n",
    "\n",
    "`sklearn.preprocessing.OneHotEncoder` <br/>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-chinese",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-router",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-combination",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_cat_1hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-anaheim",
   "metadata": {},
   "source": [
    "<table align=\"left\" class=\"dashed-box\">\n",
    "<tr>\n",
    "    <td>üí°</td>\n",
    "    <td>Notice that the output is a <i>SciPy sparse matrix</i>, instead of a <i>NumPy array</i>. This is very useful when you have categorical attributes with <b>thousands of categories</b>.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td>After one-hot encoding, we get a matrix with thousands of columns, and the matrix is <i>full of 0s</i> except for <i>a single <b>1</b> per row</i>.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td>Using up tons of memory mostly to store zeros would be very wasteful, so instead a sparse matrix only stores the location of the nonzero elements.</td>\n",
    "</tr>\n",
    "</table><br/><br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-values",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to NumPy array\n",
    "housing_train_cat_1hot.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-maldives",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the list of categories\n",
    "encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-crime",
   "metadata": {},
   "source": [
    "### 5.8. Creating Preprocessing `Pipelines`\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-empty",
   "metadata": {},
   "source": [
    "<table align=\"left\" class=\"dashed-box\">\n",
    "<tr>\n",
    "    <td><span class='alt-tab'>alt tab</span></td>\n",
    "    <td><b>Slides:</b> Scikit-Learn Design Principles - Pipelines<br/></td>\n",
    "</tr>\n",
    "</table><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-possible",
   "metadata": {},
   "source": [
    "Let's create a **Preprocessing `Pipeline`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-angel",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-anniversary",
   "metadata": {},
   "source": [
    "#### Pipeline for numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-generic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-origin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-sacrifice",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_num_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-center",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_num_preprocessed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-consumption",
   "metadata": {},
   "source": [
    "#### Pipeline for categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-telephone",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-baking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-caution",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_cat_preprocessed.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-circular",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(housing_train_cat_preprocessed.toarray() == housing_train_cat_1hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-television",
   "metadata": {},
   "source": [
    "### 5.9. Putting it all by `ColumnTransformer`\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-restaurant",
   "metadata": {},
   "source": [
    "Applies _transformers_ to **columns** of an array or pandas DataFrame. <br/>\n",
    "This **estimator** allows _different columns_ or _column subsets_ of the input to be **transformed *separately*** and the _features generated_ by each transformer will be _concatenated_ to form a **single feature space**. <br/>\n",
    "\n",
    "This is useful for _heterogeneous or columnar data_, to combine several feature extraction mechanisms or transformations into a single transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-fault",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-framing",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-tower",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-intake",
   "metadata": {},
   "source": [
    "<table align=\"left\" class=\"dashed-box\">\n",
    "<tr>\n",
    "    <td>‚ö†Ô∏è</td>\n",
    "    <td><b>BE CAREFUL</b>.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td>When performing the pipeline <i>\"numerical\"</i>, <code>ColumnTransformer</code> first <i>selects/filters</i> the columns passed by the list <code>num_attributes</code>. We then have a <i>new dataframe</i> with <b>new indices</b> that will be processed.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td>When generating new features, our custom transformer <code>HousingFeatEngineering()</code> assumes a given values for the indices of <code>total_rooms</code>, <code>total_bedrooms</code>, etc.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td>These considered indices <b>MUST MATCH EXACTLY</b> with the <i>corresponding columns</i> of the numpy array or dataframe passed as input. For our case, this matching is true.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td>But, <b>BE CAREFUL!!!</b></td>\n",
    "</tr>\n",
    "</table><br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-lottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_pre_npy = preprocessed_pipeline.fit_transform(housing_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-miami",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_pre_npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-dating",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_pipeline.named_transformers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-cooking",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_pipeline.transformers_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-trinity",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "supposed-accounting",
   "metadata": {},
   "source": [
    "### 5.10. Saving the Preprocessed Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-silly",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(preprocessed_pipeline, './models/preprocessed_pipeline.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-interest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load the pipeline\n",
    "loaded_preprocessed_pipeline = joblib.load('./models/preprocessed_pipeline.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-lighting",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_pre_npy_2 = loaded_preprocessed_pipeline.fit_transform(housing_train)\n",
    "housing_train_pre_npy_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moral-kernel",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(housing_train_pre_npy == housing_train_pre_npy_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-marketplace",
   "metadata": {},
   "source": [
    "### 5.11. Saving the Preprocessed Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./datasets/housing_train_pre_numpy_sprint-4.npy', housing_train_pre_npy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-terrace",
   "metadata": {},
   "source": [
    "## üèãÔ∏è‚Äç‚ôÄÔ∏è 6. Train ML Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-challenge",
   "metadata": {},
   "source": [
    "### 6.1. Getting the independent (features) and dependent variables (outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = housing_train_pre_npy\n",
    "# we already have y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-stranger",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-period",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-baseball",
   "metadata": {},
   "source": [
    "### 6.2. Training the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-windows",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #ff5757 !important\"><b>Cross-validation</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-principle",
   "metadata": {},
   "source": [
    "#### **‚Üí Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-abortion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()  # default parameters\n",
    "lin_scores = cross_val_score(lin_reg, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-lyric",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing function\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-jacket",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_scores(lin_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-cornwall",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "We have exactly the results of Sprint #3.\n",
    "- **Linear Regression:** \\\\$58,371.04 ¬± \\$1,757.91"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-credit",
   "metadata": {},
   "source": [
    "#### Training the final model\n",
    "After cross-validation, we can train our models by using the **entire** _training set_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-wrapping",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-database",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "muslim-plymouth",
   "metadata": {},
   "source": [
    "## üî¨üß™ 7. Evaluation on the Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-drink",
   "metadata": {},
   "source": [
    "<table align=\"left\" class=\"dashed-box\">\n",
    "<tr>\n",
    "    <td>‚ö†Ô∏è</td>\n",
    "    <td>We should to evaluate <b>many other</b> <i>quick-and-dirty models</i> before any evaluation on the test set.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td>The strategy is to select the <i>most promising models</i> and <i>fine-tune them</i> (e.g., perform grid-search to find the best hyperparameters and/or try ensemble methods. The selected models could then be evaluated in the test set.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td>We opted for evaluating our single linear regression model just to complete the end-to-end pipeline in these early sprints. We will perform the above strategy in the next sprints.</td>\n",
    "</tr>\n",
    "</table><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-attitude",
   "metadata": {},
   "source": [
    "### 7.1. Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-donor",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the testing set\n",
    "housing_test = pd.read_csv('./datasets/housing_test_sprint-2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Separate the _features_ and the _target outcome_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-drive",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocess the Test Set\n",
    "\n",
    "\n",
    "# preprocess the test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-mississippi",
   "metadata": {},
   "source": [
    "### 7.2. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-anatomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "### loading the trained model\n",
    "\n",
    "\n",
    "### evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-registration",
   "metadata": {},
   "source": [
    "### 7.3. Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-economics",
   "metadata": {},
   "source": [
    "#### RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advised-buyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "### computing the final score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lin_rmse_test = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "print(f'RMSE Lin. Reg. in the Test Set: {lin_rmse_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-peripheral",
   "metadata": {},
   "source": [
    "By using our linear regression, the **RMSE** for the Test Set -- which has never been seen/used before -- is **\\\\$ 59,439.63**. <br/>\n",
    "This error is _slightly higher_ than the _cross-validation error score_ **\\\\$58,371.04 ¬± \\$1,757.91**, which tends to be common specially when fine-tunning the hyperparameters.\n",
    "\n",
    "We need now to compare solution with the _current baseline_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-playlist",
   "metadata": {},
   "source": [
    "#### Confidence Interval for Squared Errors\n",
    "In some cases, such a _point estimate_ of the **generalization error** will not be quite enough to convince you to launch: what if it is just _0.1%_ better than the model currently in production? <br/>\n",
    "You might want to have an idea of how precise this estimate is.\n",
    "\n",
    "For this, you can compute a ***95% confidence interval*** for the generalization error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-forge",
   "metadata": {},
   "source": [
    "https://github.com/xavecoding/IFSP-CMP-D1AED-2021.1/blob/main/data_distributions/data_distributions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3435e647-2631-479e-a850-43a81ff6ee82",
   "metadata": {},
   "source": [
    "<img src='./imgs/confidence_interval.png' />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-purple",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-sudan",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-religious",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-macedonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "## alternatively\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-shadow",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# alpha ==> confidence level\n",
    "# loc ==> sample mean\n",
    "# scale ==> standard error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-claim",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the sqrt to keep the erros in the same units\n",
    "np.sqrt(confidence_interval_squared_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-receipt",
   "metadata": {},
   "source": [
    "Therefore, we have 95% of confidence that the interval \\[\\\\$56,281.32, \\\\$62,438.39]\\] contains the population generalization error mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-paintball",
   "metadata": {},
   "source": [
    "### 7.4. Comparing our model with the Baseline\n",
    "Let's first recover the description of the **baseline** from Sprint #1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-jungle",
   "metadata": {},
   "source": [
    "#### **Baseline:**\n",
    "Currently, the **district housing prices** are estimated ***manually by experts***: a team gathers up-to-date information about a district and finds out the _median housing price_. \n",
    "This is _costly_ and _time-consuming_, and their **estimates are not great**; they often realize that **their estimates were off by more than 20%**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-asbestos",
   "metadata": {},
   "source": [
    "Note that this description is a bit vague. We only have an approximation: their estimates were off by more than 20%. <br/>\n",
    "We do not have a concrete **error** for the baseline. <br/>\n",
    "\n",
    "To overcome this, we will consider that the baseline estimates final housing prices between **20% and 25% more** than they actually are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-reminder",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-mercury",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_baseline = []\n",
    "\n",
    "for true_housing_price in y_test:\n",
    "    error_rate = 1 + np.random.randint(20, 26) / 100\n",
    "    y_test_pred_baseline.append(true_housing_price * error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-summary",
   "metadata": {},
   "source": [
    "#### RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-river",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_rmse_test = mean_squared_error(y_test, y_test_pred_baseline, squared=False)\n",
    "print(f'RMSE Baseline in the Test Set: {baseline_rmse_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-cooperative",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-session",
   "metadata": {},
   "source": [
    "The final performance of our linear regression model (**\\\\$ 59,439.63**) is not better than the experts‚Äô price estimates (**\\\\$47,911.00**), which were often off by about 20%. Therefore, it is not prepared to launch in production. We need to find a better model.\n",
    "\n",
    "We may follow some strategies to find a better model than our current one:\n",
    "- Evaluate many other different models/algorithms (_e.g.,_ Polynomial regression, KNN regression, SVM regression, ...)\n",
    "- Apply some feature selection method;\n",
    "- Perform fine-tunning to find the best hyperparaments\n",
    "- Try ensemble methods\n",
    "\n",
    "After all, a model with a score similar to the baseline might be enough. Even though it is not more accurate (or with a lower error) than the baseline, the fact that the model is automatic will frees up some time for the experts so they can work on more interesting and productive tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
