{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "marine-dietary",
   "metadata": {},
   "source": [
    "### **D2APR: Aprendizado de M√°quina e Reconhecimento de Padr√µes** (IFSP, Campinas) <br/>\n",
    "**Prof. Dr.**: Samuel Martins (Samuka) <br/>\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>. <br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-morrison",
   "metadata": {},
   "source": [
    "### Custom CSS style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-catalog",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".dashed-box {\n",
    "    border: 1px dashed black !important;\n",
    "}\n",
    ".dashed-box tr {\n",
    "  background-color: white !important;  \n",
    "}\n",
    ".alt-tab {\n",
    "    background-color: black;\n",
    "    color: #ffc351;\n",
    "    padding: 4px;\n",
    "    font-size: 1em;\n",
    "    font-weight: bold;\n",
    "    font-family: monospace;\n",
    "}\n",
    "// add your CSS styling here\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-kinase",
   "metadata": {},
   "source": [
    "<span style='font-size: 2.5em'><b>California Housing üè°</b></span><br/>\n",
    "<span style='font-size: 1.5em'>Predict the median housing price in California districts</span>\n",
    "\n",
    "<span style=\"background-color: #ffc351; padding: 4px; font-size: 1em;\"><b>Sprint 6</b></span>\n",
    "\n",
    "<img src=\"./imgs/california-flag.png\" width=300/>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-conjunction",
   "metadata": {},
   "source": [
    "## Before starting this notebook\n",
    "This jupyter notebook is designed for **experimental and teaching purposes**. <br/>\n",
    "Although it is (relatively) well organized, it aims at solving the _target problem_ by evaluating (and documenting) _different solutions_ for somes steps of the **machine learning pipeline** ‚Äî see the ***Machine Learning Project Checklist by xavecoding***. <br/>\n",
    "We tried to make this notebook as literally a _notebook_. Thus, it contains notes, drafts, comments, etc.<br/>\n",
    "\n",
    "For teaching purposes, some parts of the notebook may be _overcommented_. Moreover, to simulate a real development scenario, we will divide our solution and experiments into **\"sprints\"** in which each sprint has some goals (e.g., perform _feature selection_, train more ML models, ...). <br/>\n",
    "The **sprint goal** will be stated at the beginning of the notebook.\n",
    "\n",
    "A ***final notebook*** (or any other kind of presentation) that compiles and summarizes all sprints ‚Äî the target problem, solutions, and findings ‚Äî should be created later.\n",
    "\n",
    "#### Conventions\n",
    "\n",
    "<ul>\n",
    "    <li>üí° indicates a tip. </li>\n",
    "    <li> ‚ö†Ô∏è indicates a warning message. </li>\n",
    "    <li><span class='alt-tab'>alt tab</span> indicates and an extra content (<i>e.g.</i>, slides) to explain a given concept.</li>\n",
    "</ul>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-clause",
   "metadata": {},
   "source": [
    "## üéØ Sprint Goals\n",
    "- Fine-tune the _hyperparameters_ of the Polynomial Regression models form Sprint #5.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-sussex",
   "metadata": {},
   "source": [
    "### 0. Imports and default settings for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-brighton",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "          'figure.figsize': (15, 5),\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-firmware",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è 5. Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-corrections",
   "metadata": {},
   "source": [
    "We will consider the same two scenarios for **Polynomial Regression** from Sprint #5 in this sprint:\n",
    "1. Use _only_ the `median_income`.\n",
    "2. Use _all features_ except those that generated the aggregate features (`total_rooms`, `total_bedrooms`, `population`, `household`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-massage",
   "metadata": {},
   "source": [
    "### 5.1. Load the cleaned training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-terminology",
   "metadata": {},
   "source": [
    "Let's consider the training and testing sets already cleaned (sprint #2):\n",
    "- Drop duplicated instances (no found)\n",
    "- Drop instances with `housing_median_age` capped at 52\n",
    "- Drop instances with `median_house_value` capped at 500001.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-petersburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the cleaned training set\n",
    "housing_train = pd.read_csv('./datasets/housing_train_sprint-2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-collection",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-corrections",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-thickness",
   "metadata": {},
   "source": [
    "### 5.2. Separate the _features_ and the _target outcome_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-efficiency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the target outcome into a numpy array\n",
    "y_train = housing_train['median_house_value'].values\n",
    "\n",
    "# overwrite the dataframe with only the features  \n",
    "housing_train = housing_train.drop(columns=['median_house_value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-silver",
   "metadata": {},
   "source": [
    "### 5.3. Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-mexican",
   "metadata": {},
   "source": [
    "For the sake of simplicity, let's include the **Polynomial Regression** objects (`PolynomialFeatures()` + `LinearRegression()`) into our **pipeline**. So, it is no longer _just_ dedicated to preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-speech",
   "metadata": {},
   "source": [
    "#### **Scenario 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-gender",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline for numerical\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "attributes_scenario_1 = ['median_income']\n",
    "\n",
    "pipeline_scenario_1 = Pipeline([\n",
    "    ('imputer', SimpleImputer()),   # let's evaluate the mean and median inputation\n",
    "    ('poly', PolynomialFeatures()), \n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "# we will just use the ColumnTransformer because it automaticaly filters the required columns for us before performing the pipeline.\n",
    "# (name, transformer, columns)\n",
    "preprocessed_pipeline_scenario_1 = ColumnTransformer([\n",
    "    (\"numerical\", pipeline_scenario_1, attributes_scenario_1)\n",
    "])\n",
    "\n",
    "\n",
    "# full pipeline: preprocessing + model training/prediction\n",
    "full_pipeline_scenario_1 = Pipeline([\n",
    "        ('preprocessing', preprocessed_pipeline_scenario_1),\n",
    "        ('lin_regression', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-sherman",
   "metadata": {},
   "source": [
    "#### **Scenario 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-waste",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### feature engineering method from the Sprint #4\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# our 3 new features are based on some the features: totalrooms, \n",
    "# column index\n",
    "rooms_col_idx, bedrooms_col_idx, population_col_idx, households_col_idx = 3, 4, 5, 6\n",
    "\n",
    "class HousingFeatEngineering(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    \n",
    "    def transform(self, X):\n",
    "        n_rows = X.shape[0]\n",
    "        \n",
    "        # creating the new features\n",
    "        rooms_per_household = X[:, rooms_col_idx] / X[:, households_col_idx]\n",
    "        population_per_household = X[:, population_col_idx] / X[:, households_col_idx]\n",
    "        bedrooms_per_room = X[:, bedrooms_col_idx] / X[:, rooms_col_idx]\n",
    "                \n",
    "        # to concatenate the new array as columns in our feature matrix, we need to reshape them first\n",
    "        rooms_per_household = rooms_per_household.reshape((n_rows, 1))\n",
    "        population_per_household = population_per_household.reshape((n_rows, 1))\n",
    "        bedrooms_per_room = bedrooms_per_room.reshape((n_rows, 1))\n",
    "        \n",
    "        # concatenating the new features into the feature matrix\n",
    "        X_out = np.hstack((X, rooms_per_household, population_per_household, bedrooms_per_room))\n",
    "        \n",
    "        return X_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-firmware",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# our 3 new features are based on some the features: totalrooms, \n",
    "# column index\n",
    "rooms_col_idx, bedrooms_col_idx, population_col_idx, households_col_idx = 3, 4, 5, 6\n",
    "\n",
    "class DropFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, verbose=False):\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_out = np.delete(X, [rooms_col_idx, bedrooms_col_idx, population_col_idx, households_col_idx], axis=1)\n",
    "        \n",
    "        # for debugging\n",
    "        if self.verbose:\n",
    "            np.set_printoptions(suppress=True)\n",
    "            print('X[:5]')\n",
    "            print(X[:5])\n",
    "            print('\\nX_out[:5]')\n",
    "            print(X_out[:5])\n",
    "        \n",
    "        return X_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-crown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline for numerical\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "attributes_scenario_2 = housing_train.columns.drop('ocean_proximity')\n",
    "\n",
    "pipeline_scenario_2 = Pipeline([\n",
    "    ('imputer', SimpleImputer()),  # let's evaluate the mean and median inputation\n",
    "    ('feat_engineering', HousingFeatEngineering()),\n",
    "    ('drop_features', DropFeatures(verbose=False)),\n",
    "    ('poly', PolynomialFeatures()),\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "# we will just use the ColumnTransformer because it automaticaly filters the required columns for us before performing the pipeline.\n",
    "# (name, transformer, columns)\n",
    "preprocessed_pipeline_scenario_2 = ColumnTransformer([\n",
    "    (\"numerical\", pipeline_scenario_2, attributes_scenario_2)\n",
    "])\n",
    "\n",
    "# full pipeline: preprocessing + model training/prediction\n",
    "full_pipeline_scenario_2 = Pipeline([\n",
    "        ('preprocessing', preprocessed_pipeline_scenario_2),\n",
    "        ('lin_regression', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-dollar",
   "metadata": {},
   "source": [
    "## üèãÔ∏è‚Äç‚ôÄÔ∏è 6. Train ML Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-liverpool",
   "metadata": {},
   "source": [
    "### 6.1. Grid-Search (fine-tunning)\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-emperor",
   "metadata": {},
   "source": [
    "### **Scenario 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-commodity",
   "metadata": {},
   "source": [
    "#### **Finding out the hyperparameter key names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-correspondence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "pending-indie",
   "metadata": {},
   "source": [
    "#### **Grid-search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-swift",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-aberdeen",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvres = grid_search_scenario_1.cv_results_\n",
    "\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-cooking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-enough",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best estimator\n",
    "# if refit=True, it returns a model trained with the full training set with the best hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the (approximated) RMSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-lender",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the approximated RMSE and its standard deviation\n",
    "best_index = grid_search_scenario_1.best_index_\n",
    "best_score = np.sqrt(-grid_search_scenario_1.cv_results_['mean_test_score'][best_index])\n",
    "best_score_std = np.sqrt(grid_search_scenario_1.cv_results_['std_test_score'][best_index])\n",
    "\n",
    "print(f'Best score: {best_score} +- {best_score_std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-armenia",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "The **cross validation score** from **the best model** (\\\\$71,298.70 ¬± \\\\$11,036.55) is _slightly better_ than the score of Scenario 1 without fine-tunning (\\\\$71,671.86 ¬± \\$1,479.30) from Sprint #5, its results are more unstable (see its _standard deviation_). <br/>\n",
    "Moreover, its **cross validation score** keeps being _considerably higher_ than that of `Linear Regression` from the previous Sprint (\\\\$58,371.04 ¬± \\$1,757.91)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-shield",
   "metadata": {},
   "source": [
    "<table align=\"left\" class=\"dashed-box\">\n",
    "<tr>\n",
    "    <td>üí°</td>\n",
    "    <td>Mathematically, the correct way to estimate the <b>cross-validation RMSE</b> and its <b>standard deviation</b> is to apply the <i>square root</i> for the each split <i>negative MSE score</i>, and then compute the <i>mean</i> and <i>std</i> with these new scores.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td>For that, we need to 'debug' the _cross validation results.</td>\n",
    "</tr>\n",
    "</table><br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-captain",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_scenario_1.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-production",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "split_keys = [f'split{i}_test_score' for i in range(n_folds)]\n",
    "best_index = grid_search_scenario_1.best_index_\n",
    "\n",
    "rmse_scores = []\n",
    "\n",
    "for key in split_keys:\n",
    "    neg_mse_score = grid_search_scenario_1.cv_results_[key][best_index]\n",
    "    rmse_scores.append(np.sqrt(-neg_mse_score))\n",
    "\n",
    "best_rmse = np.mean(rmse_scores)\n",
    "best_rmse_std = np.std(rmse_scores)\n",
    "\n",
    "\n",
    "print(f'Best RMSE score: {best_rmse} +- {best_rmse_std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-bryan",
   "metadata": {},
   "source": [
    "By doing it in the _mathematically correct way_, we see that the **true mean RMSE** (\\\\$71,293.59) is very close from the approximated one (\\\\$71,298.70). <br/>\n",
    "However, the **true standard deviation for RMSE** (\\$854.29) is _considerably smaller_ than the approximated one (\\\\$11,036.55)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-airplane",
   "metadata": {},
   "source": [
    "### **Scenario 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-sierra",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline_scenario_2.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-intent",
   "metadata": {},
   "source": [
    "Since our feature matrix has a considerable number of features, for computation time reasons, we decided to:\n",
    "- Consider a small number of hyperparameter combinations (small search space)\n",
    "- Low polynomial degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-update",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid_scenario_2 = [\n",
    "    {\n",
    "     'preprocessing__numerical__imputer__strategy': ['mean', 'median'],\n",
    "     'preprocessing__numerical__poly__degree': [2, 3, 4, 5],\n",
    "     'preprocessing__numerical__poly__include_bias': [False, True],\n",
    "     'preprocessing__numerical__poly__interaction_only': [False]\n",
    "    }\n",
    "]\n",
    "\n",
    "grid_search_scenario_2 = GridSearchCV(full_pipeline_scenario_2, param_grid_scenario_2, cv=5, scoring='neg_mean_squared_error', return_train_score=True, verbose=1)\n",
    "grid_search_scenario_2.fit(housing_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-mozambique",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvres = grid_search_scenario_2.cv_results_\n",
    "\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-negative",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_scenario_2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-cannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if refit=True, it returns a model trained with the full training set with the best hyperparameters\n",
    "best_model = grid_search_scenario_2.best_estimator_\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-launch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best approximated RMSE\n",
    "np.sqrt(-grid_search_scenario_2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-administration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the approximated RMSE and its standard deviation\n",
    "best_index = grid_search_scenario_2.best_index_\n",
    "best_score = np.sqrt(-grid_search_scenario_2.cv_results_['mean_test_score'][best_index])\n",
    "best_score_std = np.sqrt(grid_search_scenario_2.cv_results_['std_test_score'][best_index])\n",
    "\n",
    "print(f'Best score: {best_score} +- {best_score_std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-startup",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "split_keys = [f'split{i}_test_score' for i in range(n_folds)]\n",
    "best_index = grid_search_scenario_2.best_index_\n",
    "\n",
    "rmse_scores = []\n",
    "\n",
    "for key in split_keys:\n",
    "    neg_mse_score = grid_search_scenario_2.cv_results_[key][best_index]\n",
    "    rmse_scores.append(np.sqrt(-neg_mse_score))\n",
    "\n",
    "best_rmse = np.mean(rmse_scores)\n",
    "best_rmse_std = np.std(rmse_scores)\n",
    "\n",
    "\n",
    "print(f'Best RMSE score: {best_rmse} +- {best_rmse_std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-aviation",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "The large **best true RMSE** (\\\\$345,172.71) -- even larger for the approximated one (\\\\$626,165.23) -- shows that (i) this model is not suitable for the problem, and/or (ii) the considered _search space_ is insufficient to get better models. <br/>\n",
    "\n",
    "We'd better We'd better throw this model away."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
