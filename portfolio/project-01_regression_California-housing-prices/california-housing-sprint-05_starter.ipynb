{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "flush-anger",
   "metadata": {},
   "source": [
    "### **D2APR: Aprendizado de M√°quina e Reconhecimento de Padr√µes** (IFSP, Campinas) <br/>\n",
    "**Prof**: Samuel Martins (Samuka) <br/>\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>. <br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-amino",
   "metadata": {},
   "source": [
    "### Custom CSS style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-exhaust",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".dashed-box {\n",
    "    border: 1px dashed black !important;\n",
    "}\n",
    ".dashed-box tr {\n",
    "  background-color: white !important;  \n",
    "}\n",
    ".alt-tab {\n",
    "    background-color: black;\n",
    "    color: #ffc351;\n",
    "    padding: 4px;\n",
    "    font-size: 1em;\n",
    "    font-weight: bold;\n",
    "    font-family: monospace;\n",
    "}\n",
    "// add your CSS styling here\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-cornell",
   "metadata": {},
   "source": [
    "<span style='font-size: 2.5em'><b>California Housing üè°</b></span><br/>\n",
    "<span style='font-size: 1.5em'>Predict the median housing price in California districts</span>\n",
    "\n",
    "<span style=\"background-color: #ffc351; padding: 4px; font-size: 1em;\"><b>Sprint 5</b></span>\n",
    "\n",
    "<img src=\"./imgs/california-flag.png\" width=300/>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divine-reverse",
   "metadata": {},
   "source": [
    "## Before starting this notebook\n",
    "This jupyter notebook is designed for **experimental and teaching purposes**. <br/>\n",
    "Although it is (relatively) well organized, it aims at solving the _target problem_ by evaluating (and documenting) _different solutions_ for somes steps of the **machine learning pipeline** ‚Äî see the ***Machine Learning Project Checklist by xavecoding***. <br/>\n",
    "We tried to make this notebook as literally a _notebook_. Thus, it contains notes, drafts, comments, etc.<br/>\n",
    "\n",
    "For teaching purposes, some parts of the notebook may be _overcommented_. Moreover, to simulate a real development scenario, we will divide our solution and experiments into **\"sprints\"** in which each sprint has some goals (e.g., perform _feature selection_, train more ML models, ...). <br/>\n",
    "The **sprint goal** will be stated at the beginning of the notebook.\n",
    "\n",
    "A ***final notebook*** (or any other kind of presentation) that compiles and summarizes all sprints ‚Äî the target problem, solutions, and findings ‚Äî should be created later.\n",
    "\n",
    "#### Conventions\n",
    "\n",
    "<ul>\n",
    "    <li>üí° indicates a tip. </li>\n",
    "    <li> ‚ö†Ô∏è indicates a warning message. </li>\n",
    "    <li><span class='alt-tab'>alt tab</span> indicates and an extra content (<i>e.g.</i>, slides) to explain a given concept.</li>\n",
    "</ul>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-terrorism",
   "metadata": {},
   "source": [
    "## üéØ Sprint Goals\n",
    "- Evaluate Polynomial Regression Models\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-messenger",
   "metadata": {},
   "source": [
    "### 0. Imports and default settings for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-sponsorship",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "          'figure.figsize': (15, 5),\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-english",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è 5. Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-printer",
   "metadata": {},
   "source": [
    "To use **Polynomial Regression**, we need to decide _which features_ will be used/considered first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-request",
   "metadata": {},
   "source": [
    "### 5.1. Load the cleaned training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-statement",
   "metadata": {},
   "source": [
    "Let's consider the training and testing sets already cleaned (sprint #2):\n",
    "- Drop duplicated instances (no found)\n",
    "- Drop instances with `housing_median_age` capped at 52\n",
    "- Drop instances with `median_house_value` capped at 500001.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the cleaned training set\n",
    "housing_train = pd.read_csv('./datasets/housing_train_sprint-2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-internship",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-mambo",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-bahrain",
   "metadata": {},
   "source": [
    "### 5.2. Quick EDA to get insights about the features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-africa",
   "metadata": {},
   "source": [
    "#### **Generate aggregate features**\n",
    "Let's also analyse the new features created in the previous sprints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-accent",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_eda = housing_train.copy()\n",
    "\n",
    "housing_train_eda[\"rooms_per_household\"] = housing_train_eda[\"total_rooms\"] / housing_train_eda[\"households\"]\n",
    "housing_train_eda[\"bedrooms_per_room\"] = housing_train_eda[\"total_bedrooms\"] / housing_train_eda[\"total_rooms\"]\n",
    "housing_train_eda[\"population_per_household\"] = housing_train_eda[\"population\"] / housing_train_eda[\"households\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-solid",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-average",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_eda.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-pepper",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(housing_train_eda, x_vars=['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income'],  y_vars=['median_house_value'], height=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-karaoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(housing_train_eda, x_vars=['rooms_per_household', 'bedrooms_per_room', 'population_per_household'],  y_vars=['median_house_value'], height=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-service",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "By looking at the scatter plots, we cannot identify a specific relationship (linear, quadratic, cubic, ...) between the _features_ and the _outcome_ (`median_house_value`). <br/>\n",
    "As observersed in previous sprints, the `median_income` seems to have a _'linear'_ relationship with the `median_house_value`.\n",
    "\n",
    "The `population_per_household` has significative _outliers_ which impair its visualization. Let's **remove** these outliers to try to improve the visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-bidder",
   "metadata": {},
   "source": [
    "##### **Removing outliers for `population_per_household`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-client",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-corrections",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR outlier detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-scanning",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(housing_train_eda_without_outliers, x_vars=['rooms_per_household', 'bedrooms_per_room', 'population_per_household'],  y_vars=['median_house_value'], height=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-possibility",
   "metadata": {},
   "source": [
    "Now, we can see the true dispersion of the `population_per_household` and the `median_house_value`. However there is not a clear relationship between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-paradise",
   "metadata": {},
   "source": [
    "Let's then consider two scenarios for **Polynomial Regression**\n",
    "1. Use _only_ the `median_income`.\n",
    "2. Use _all features_ except those that generated the aggregate features (`total_rooms`, `total_bedrooms`, `population`, `household`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-celtic",
   "metadata": {},
   "source": [
    "### 5.3. Separate the _features_ and the _target outcome_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-composer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the target outcome into a numpy array\n",
    "y_train = housing_train['median_house_value'].values\n",
    "\n",
    "# overwrite the dataframe with only the features  \n",
    "housing_train = housing_train.drop(columns=['median_house_value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-father",
   "metadata": {},
   "source": [
    "### 5.4. Preprocessing Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-karen",
   "metadata": {},
   "source": [
    "It is indicated to apply the _polynomial feature transformation_ **before** _feature scaling_. <br/>\n",
    "There are two good reasons for that:\n",
    "\n",
    "**1. Loss of signal**\n",
    "\n",
    "When creating _feature interactions_, we generate values that are _multiples_, _squares_, _cubics_, etc, _ of themselves. <br/>\n",
    "When we perform _feature scaling_, we convert original values (signals) to a given scale, for example, between -3 and +3 for _standardization_. <br/>\n",
    "\n",
    "Creating _polynomial features_ from these scaled data will get values with a _magnitude **smaller** than the original_. <br/>\n",
    "For example, imagine generating polynomial features from values between 0 and 1 by each other. We can only end up with more values between 0 and 1.\n",
    "\n",
    "The purpose of **Polynomial Feature Transformation** is to **increase signal**. To retain this _increased signal_, we had better generate the polynomial features **first** then apply feature scaling. <br/><br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-local",
   "metadata": {},
   "source": [
    "**2. Making random negatives**\n",
    "\n",
    "Depending on the feature scaling strategy, we can turn a set of _only positive values_ into **positive and negative values** (for example, _standardization_). <br/>\n",
    "When multiplying _negative_ by _positive_, during _polynomial feature transformation_, you get **negative**.\n",
    "\n",
    "Following this to our data will create **negative values** from previously **all-positive values**. <br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-links",
   "metadata": {},
   "source": [
    "_Source: https://samchaaa.medium.com/preprocessing-why-you-should-generate-polynomial-features-first-before-standardizing-892b4326a91d_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-production",
   "metadata": {},
   "source": [
    "<table align=\"left\" class=\"dashed-box\">\n",
    "<tr>\n",
    "    <td>üí°</td>\n",
    "    <td>For this initial evaluation, let's consider the <b>default parameters</b> of <code>PolynomialFeatures</code> but the <code>include_bias</code>.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html\">https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html</a></td>\n",
    "</tr>\n",
    "</table><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-freight",
   "metadata": {},
   "source": [
    "#### **Scenario 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-mother",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline for numerical\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-guarantee",
   "metadata": {},
   "source": [
    "#### **Scenario 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thousand-drill",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attributes = housing_train.columns.drop('ocean_proximity')\n",
    "num_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-cruise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the integer index of each attribute/column for the filtered dataframe by the numeric attributes:\n",
    "for index, column_name in enumerate(housing_train[num_attributes]):\n",
    "    print(f'{index} = {column_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-shower",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### feature engineering method from the Sprint #4\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# our 3 new features are based on some the features: totalrooms, \n",
    "# column index\n",
    "rooms_col_idx, bedrooms_col_idx, population_col_idx, households_col_idx = 3, 4, 5, 6\n",
    "\n",
    "class HousingFeatEngineering(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    \n",
    "    def transform(self, X):\n",
    "        n_rows = X.shape[0]\n",
    "        \n",
    "        # creating the new features\n",
    "        rooms_per_household = X[:, rooms_col_idx] / X[:, households_col_idx]\n",
    "        population_per_household = X[:, population_col_idx] / X[:, households_col_idx]\n",
    "        bedrooms_per_room = X[:, bedrooms_col_idx] / X[:, rooms_col_idx]\n",
    "                \n",
    "        # to concatenate the new array as columns in our feature matrix, we need to reshape them first\n",
    "        rooms_per_household = rooms_per_household.reshape((n_rows, 1))\n",
    "        population_per_household = population_per_household.reshape((n_rows, 1))\n",
    "        bedrooms_per_room = bedrooms_per_room.reshape((n_rows, 1))\n",
    "        \n",
    "        # concatenating the new features into the feature matrix\n",
    "        X_out = np.hstack((X, rooms_per_household, population_per_household, bedrooms_per_room))\n",
    "        \n",
    "        return X_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "durable-craps",
   "metadata": {},
   "source": [
    "The columns of **output numpy array** will correspond to: <br/>\n",
    "0 = longitude <br/>\n",
    "1 = latitude <br/>\n",
    "2 = housing_median_age <br/>\n",
    "3 = total_rooms <br/>\n",
    "4 = total_bedrooms <br/>\n",
    "5 = population <br/>\n",
    "6 = households <br/>\n",
    "7 = median_income <br/>\n",
    "8 = rooms_per_household <br/>\n",
    "9 = population_per_household <br/>\n",
    "10 = bedrooms_per_room <br/>\n",
    "\n",
    "To satisfy the scenario 2, we need to **remove/drop** the features `total_rooms`, `total_bedrooms`, `population`, `household`. <br/>\n",
    "To do this automatically, we can create another **transformer** that removes the corresponding numpy array columns after `HousingFeatEngineering` throughout their column indices. <br/>\n",
    "Coincidentally, these column indices are _the same_ used in `HousingFeatEngineering`... but, **always be aware of this.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# indices from the columns to be dropped\n",
    "rooms_col_idx, bedrooms_col_idx, population_col_idx, households_col_idx = 3, 4, 5, 6\n",
    "\n",
    "class DropFeatures(BaseEstimator, TransformerMixin):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-dispute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline for numerical\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-marketplace",
   "metadata": {},
   "source": [
    "## üèãÔ∏è‚Äç‚ôÄÔ∏è 6. Train ML Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-mailman",
   "metadata": {},
   "source": [
    "### 6.1. Getting the independent (features) and dependent variables (outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "banner-thunder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenario 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requested-paraguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_scenario_1.shape)\n",
    "print(X_train_scenario_1[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-forest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenario 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naked-intranet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we already have `y_train`\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-shelter",
   "metadata": {},
   "source": [
    "### 6.2. Training the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-render",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing function\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-contributor",
   "metadata": {},
   "source": [
    "#### **‚Üí Scenario 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-complex",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# create linear regression\n",
    "\n",
    "\n",
    "# apply cross-validation\n",
    "\n",
    "\n",
    "# compute the RMSE scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-charlotte",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_scores(lin_rmse_scores_scenario_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-uncertainty",
   "metadata": {},
   "source": [
    "Although the errors are relatively _stable_ across the folds (look at the _standard deviation_), the **cross validation score** (\\\\$71,671.86 ¬± \\$1,479.30) is _considerably higher_ than that of `Linear Regression` from the previous Sprint (\\\\$58,371.04 ¬± \\$1,757.91)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-outreach",
   "metadata": {},
   "source": [
    "<table align=\"left\" class=\"dashed-box\">\n",
    "<tr>\n",
    "    <td>üí°</td>\n",
    "    <td>Although we have created a <code>Pipeline</code> only for <b>preprocessing</b>, we could incorporate <b>all steps/models/transformers</b> (including <code>linear regression</code>) into a <b>single <code>Pipeline</code><b/>.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td>We will see that in the next sprints.</td>\n",
    "</tr>\n",
    "</table><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-complex",
   "metadata": {},
   "source": [
    "#### **‚Üí Scenario 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-temple",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# create linear regression\n",
    "\n",
    "\n",
    "# apply cross-validation\n",
    "\n",
    "\n",
    "# compute the RMSE scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frank-communication",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_scores(lin_rmse_scores_scenario_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heated-shadow",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "This **polynomial regression model** is very unstable (look at its _standard deviation_): it presents lower errors in some folds and extremely high ones in other folds. <br/>\n",
    "It seems that:\n",
    "- this combination of features is not good; and/or\n",
    "- the outliers (regardless their nature) are impacting the results; and/or\n",
    "- the considered degree is not adequate; and/or\n",
    "- this model is not adequate for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-papua",
   "metadata": {},
   "source": [
    "<table align=\"left\" class=\"dashed-box\">\n",
    "<tr>\n",
    "    <td>üí°</td>\n",
    "    <td>Before ignoring <b>polynomial regression</b> for our problem, we could try to <b><i>fine-tune</i></b> its <i>hyperparameters</i>, especially the <code>degree</code> since it highly impacts the final results, and/or remove outliers.</td>\n",
    "</tr>\n",
    "</table><br/><br/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
