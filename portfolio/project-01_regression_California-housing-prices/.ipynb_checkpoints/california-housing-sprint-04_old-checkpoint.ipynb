{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "southeast-gravity",
   "metadata": {},
   "source": [
    "### **D2APR: Aprendizado de M√°quina e Reconhecimento de Padr√µes** (IFSP, Campinas) <br/>\n",
    "**Prof**: Samuel Martins (Samuka) <br/>\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>. <br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-tuition",
   "metadata": {},
   "source": [
    "### Custom CSS style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-seven",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".dashed-box {\n",
    "    border: 1px dashed black !important;\n",
    "}\n",
    ".dashed-box tr {\n",
    "  background-color: white !important;  \n",
    "}\n",
    ".alt-tab {\n",
    "    background-color: black;\n",
    "    color: #ffc351;\n",
    "    padding: 4px;\n",
    "    font-size: 1em;\n",
    "    font-weight: bold;\n",
    "    font-family: monospace;\n",
    "}\n",
    "// add your CSS styling here\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-bangkok",
   "metadata": {},
   "source": [
    "<span style='font-size: 2.5em'><b>California Housing üè°</b></span><br/>\n",
    "<span style='font-size: 1.5em'>Predict the median housing price in California districts</span>\n",
    "\n",
    "<span style=\"background-color: #ffc351; padding: 4px; font-size: 1em;\"><b>Sprint 4</b></span>\n",
    "\n",
    "<img src=\"./imgs/california-flag.png\" width=300/>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-catalog",
   "metadata": {},
   "source": [
    "## Before starting this notebook\n",
    "This jupyter notebook is designed for **experimental and teaching purposes**. <br/>\n",
    "Although it is (relatively) well organized, it aims at solving the _target problem_ by evaluating (and documenting) _different solutions_ for somes steps of the **machine learning pipeline** ‚Äî see the ***Machine Learning Project Checklist by xavecoding***. <br/>\n",
    "We tried to make this notebook as literally a _notebook_. Thus, it contains notes, drafts, comments, etc.<br/>\n",
    "\n",
    "For teaching purposes, some parts of the notebook may be _overcommented_. Moreover, to simulate a real development scenario, we will divide our solution and experiments into **\"sprints\"** in which each sprint has some goals (e.g., perform _feature selection_, train more ML models, ...). <br/>\n",
    "The **sprint goal** will be stated at the beginning of the notebook.\n",
    "\n",
    "A ***final notebook*** (or any other kind of presentation) that compiles and summarizes all sprints ‚Äî the target problem, solutions, and findings ‚Äî should be created later.\n",
    "\n",
    "#### Conventions\n",
    "\n",
    "<ul>\n",
    "    <li>üí° indicates a tip. </li>\n",
    "    <li> ‚ö†Ô∏è indicates a warning message. </li>\n",
    "    <li><span class='alt-tab'>alt tab</span> indicates and an extra content (<i>e.g.</i>, slides) to explain a given concept.</li>\n",
    "</ul>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-plasma",
   "metadata": {},
   "source": [
    "## üéØ Sprint Goals\n",
    "- Refactor our codes by using the sklearn Pipelines\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-probability",
   "metadata": {},
   "source": [
    "### 0. Imports and default settings for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-sector",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "          'figure.figsize': (15, 5),\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-table",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è 5. Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-assistant",
   "metadata": {},
   "source": [
    "#### **Preprocessing tasks**\n",
    "- Fill in missing values (imputation)\n",
    "- Add new features\n",
    "- Feature Scaling\n",
    "- One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-pierre",
   "metadata": {},
   "source": [
    "<table align=\"left\" class=\"dashed-box\">\n",
    "<tr>\n",
    "    <td><span class='alt-tab'>alt tab</span></td>\n",
    "    <td><b>Slides:</b> Scikit-Learn Design Principles - Hyperparameters vs Parameters<br/>\n",
    "        <b>Slides:</b> Scikit-Learn Design Principles - Main APIs</td>\n",
    "</tr>\n",
    "</table><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-consideration",
   "metadata": {},
   "source": [
    "### 5.1. Load the cleaned training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-chicken",
   "metadata": {},
   "source": [
    "Let's consider the training and testing sets already cleaned (sprint #2):\n",
    "- Drop duplicated instances (no found)\n",
    "- Drop instances with `housing_median_age` capped at 52\n",
    "- Drop instances with `median_house_value` capped at 500001.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-documentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the cleaned training set\n",
    "housing_train = pd.read_csv('./datasets/housing_train_sprint-2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-snowboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-deadline",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-damages",
   "metadata": {},
   "source": [
    "### 5.2. Separate the _features_ and the _target outcome_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-difficulty",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the target outcome into a numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-humanitarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-posting",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-electric",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwrite the dataframe with only the features  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "young-museum",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-secret",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-husband",
   "metadata": {},
   "source": [
    "### 5.3. Separate the _numerical_ and _categorical_ features\n",
    "Since we perform different preprocessing tasks (transformations) to _numerical_ features and _categorical_ ones, let's split them into two different dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-duplicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-bankruptcy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical atributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-census",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-portuguese",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating the features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-german",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-necessity",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-sauce",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.4. Filling in missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepting-comedy",
   "metadata": {},
   "source": [
    "`sklearn.impute.SimpleImputer` <br/>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-stick",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-jones",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.statistics_  # computed medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-melissa",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_num.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal-audio",
   "metadata": {},
   "source": [
    "<table align=\"left\" class=\"dashed-box\">\n",
    "<tr>\n",
    "    <td>üí°</td>\n",
    "    <td>The <code>SimpleImputer</code> finds out the <i>statistic for imputation</i> <b>for ALL features</b>.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td>We can save this <i>transformer</i> on the disk for future transfomations.</td>\n",
    "</tr>\n",
    "</table><br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-stage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling in the missing values FOR ALL attributes\n",
    "# it generates a numpy array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal-exhibit",
   "metadata": {},
   "source": [
    "### 5.5. Adding new features\n",
    "To _automate data preprocessing_ via sklearn, we will need _to create_ our **own transformer** to add the new features considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-numbers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# template to create an own estimation\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class NameOfYourTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return None  # return the transformed data instead of None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-siemens",
   "metadata": {},
   "source": [
    "Since our custom transformer can be executed before other transformation, we will consider that the input is a **numpy 2D array**, not a _dataframe_. <br/>\n",
    "\n",
    "This transformer will create 3 new features, based on the current ones:\n",
    "- `total_rooms`\n",
    "- `total_bedrooms`\n",
    "- `population`\n",
    "- `households`\n",
    "\n",
    "\n",
    "Thus, we need to find their column indices first because our input will be a **numpy 2D array**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-statistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the integer index of each attribute/column:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-gardening",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-group",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_engineer = HousingFeatEngineering()\n",
    "\n",
    "housing_train_num_new_feats = feat_engineer.transform(housing_train_num.values)  # we need to convert it to numpy first\n",
    "housing_train_num_new_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-rocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_num_new_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-earth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the new feats\n",
    "housing_train_num_new_feats[:, -3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-competition",
   "metadata": {},
   "source": [
    "### 5.6. Feature Scaling\n",
    "Exactly as performed in the previous sprint: **RobustScaler**. <br/>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-requirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(housing_train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-expansion",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_num_scaled = scaler.transform(housing_train_num)\n",
    "housing_train_num_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-norway",
   "metadata": {},
   "source": [
    "### 5.7. Categorical Varaible Encoding\n",
    "Instead of using the method `.get_dummies()` from _pandas_, let's use a method from _sklearn_.\n",
    "\n",
    "`sklearn.preprocessing.OneHotEncoder` <br/>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-consequence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-ivory",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-freeware",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_cat_1hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-white",
   "metadata": {},
   "source": [
    "<table align=\"left\" class=\"dashed-box\">\n",
    "<tr>\n",
    "    <td>üí°</td>\n",
    "    <td>Notice that the output is a <i>SciPy sparse matrix</i>, instead of a <i>NumPy array</i>. This is very useful when you have categorical attributes with <b>thousands of categories</b>.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td>After one-hot encoding, we get a matrix with thousands of columns, and the matrix is <i>full of 0s</i> except for <i>a single <b>1</b> per row</i>.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td>Using up tons of memory mostly to store zeros would be very wasteful, so instead a sparse matrix only stores the location of the nonzero elements.</td>\n",
    "</tr>\n",
    "</table><br/><br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-springfield",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to NumPy array\n",
    "housing_train_cat_1hot.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-button",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the list of categories\n",
    "encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-affect",
   "metadata": {},
   "source": [
    "### 5.8. Creating Preprocessing `Pipelines`\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-opening",
   "metadata": {},
   "source": [
    "<table align=\"left\" class=\"dashed-box\">\n",
    "<tr>\n",
    "    <td><span class='alt-tab'>alt tab</span></td>\n",
    "    <td><b>Slides:</b> Scikit-Learn Design Principles - Pipelines<br/></td>\n",
    "</tr>\n",
    "</table><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-madagascar",
   "metadata": {},
   "source": [
    "Let's create a **Preprocessing `Pipeline`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-artwork",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-librarian",
   "metadata": {},
   "source": [
    "#### Pipeline for numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-nothing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-brunswick",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-nebraska",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_num_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-herald",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_num_preprocessed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-consistency",
   "metadata": {},
   "source": [
    "#### Pipeline for categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-hygiene",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-reserve",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-electricity",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_cat_preprocessed.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-slave",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(housing_train_cat_preprocessed.toarray() == housing_train_cat_1hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-season",
   "metadata": {},
   "source": [
    "### 5.9. Putting it all by `ColumnTransformer`\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-lying",
   "metadata": {},
   "source": [
    "Applies _transformers_ to **columns** of an array or pandas DataFrame. <br/>\n",
    "This **estimator** allows _different columns_ or _column subsets_ of the input to be **transformed *separately*** and the _features generated_ by each transformer will be _concatenated_ to form a **single feature space**. <br/>\n",
    "\n",
    "This is useful for _heterogeneous or columnar data_, to combine several feature extraction mechanisms or transformations into a single transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-decision",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-eight",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-hostel",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-being",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-pleasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_pre_npy = full_pipeline.fit_transform(housing_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worse-header",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_pre_npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-pledge",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline.named_transformers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-replica",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline.transformers_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-survival",
   "metadata": {},
   "source": [
    "### 5.10. Saving the Preprocessed Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defensive-seminar",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(full_pipeline, 'full_pipeline.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-representation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load the pipeline\n",
    "loaded_full_pipeline = joblib.load('full_pipeline.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-method",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train_pre_npy_2 = loaded_full_pipeline.fit_transform(housing_train)\n",
    "housing_train_pre_npy_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-advancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(housing_train_pre_npy == housing_train_pre_npy_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-directive",
   "metadata": {},
   "source": [
    "### 5.11. Saving the Preprocessed Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-prague",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./datasets/housing_train_pre_numpy_sprint-4.npy', housing_train_pre_npy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-landscape",
   "metadata": {},
   "source": [
    "## üèãÔ∏è‚Äç‚ôÄÔ∏è 6. Train ML Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-strip",
   "metadata": {},
   "source": [
    "### 6.1. Getting the independent (features) and dependent variables (outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-piece",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = housing_train_pre_npy\n",
    "# we already have y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-vacuum",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-spiritual",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-singing",
   "metadata": {},
   "source": [
    "### 6.2. Training the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-round",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #ff5757 !important\"><b>Cross-validation</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-genealogy",
   "metadata": {},
   "source": [
    "#### **‚Üí Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-variation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()  # default parameters\n",
    "lin_scores = cross_val_score(lin_reg, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-announcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing function\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-florida",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_scores(lin_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-training",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "We have exactly the results Sprint #3.\n",
    "- **Linear Regression:** \\\\$58,371 ¬± \\$1,757"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
